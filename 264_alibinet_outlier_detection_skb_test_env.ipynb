{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf gpu with alibi_detect environment setup\n",
    "```bash\n",
    "conda create --name=tf python=3.9 \n",
    "conda activate tf\n",
    "conda install -c conda-forge cudatoolkit=11.2.2 cudnn=8.1.0\n",
    "\n",
    "mkdir -p $CONDA_PREFIX/etc/conda/activate.d\n",
    "echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/' > $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\n",
    "exit\n",
    "\n",
    "#login again\n",
    "\n",
    "conda activate tf\n",
    "python3 -m pip install tensorflow==2.10\n",
    "python3 -c \"import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'; import tensorflow as tf; print('Num GPUs Available: ', len(tf.config.list_physical_devices('GPU')))\"\n",
    "pip install alibi-detect\n",
    "pip install alibi-detect[tensorflow]\n",
    "pip install tensorflow_probability==0.12.2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 05:25:09.045438: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-05 05:25:09.070844: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-05 05:25:09.071306: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-05 05:25:09.597794: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skb/miniconda3/envs/ad2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KeOps] Warning : \n",
      "    The default C++ compiler could not be found on your system.\n",
      "    You need to either define the CXX environment variable or a symlink to the g++ command.\n",
      "    For example if g++-8 is the command you can do\n",
      "      import os\n",
      "      os.environ['CXX'] = 'g++-8'\n",
      "    \n",
      "[KeOps] Warning : Cuda libraries were not detected on the system ; using cpu only mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "# %env TF_CPP_MIN_LOG_LEVEL=2\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Reshape, InputLayer, Flatten\n",
    "\n",
    "from alibi_detect.od import OutlierAE, OutlierVAE\n",
    "from alibi_detect.utils.visualize import plot_instance_score, plot_feature_outlier_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug note\n",
    "https://github.com/huggingface/transformers/issues/18549\n",
    "pip uninstall tokenizers\n",
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 128, 128, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################################################\n",
    "#Load data. We only need good data and anything NOT good is an outlier. \n",
    "\n",
    "image_directory = './datasets/'\n",
    "SIZE = 128\n",
    "dataset = []  #Many ways to handle data, you can use pandas. Here, we are using a list format.  \n",
    "\n",
    "good_images = os.listdir(image_directory + 'good/')\n",
    "for i, image_name in enumerate(good_images[:900]):\n",
    "    # print (image_name)\n",
    "    if (image_name.split('.')[-1] == 'jpg'):\n",
    "        image = cv2.imread(image_directory + 'good/' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "    \n",
    "\n",
    "dataset = np.array(dataset)\n",
    "\n",
    "train = dataset[0:10]\n",
    "test = dataset[11:15]\n",
    "\n",
    "train = train.astype('float32') / 255.\n",
    "test = test.astype('float32') / 255.\n",
    "\n",
    "#Let us also load bad images to verify our trained model. \n",
    "bad_images = os.listdir(image_directory + 'bad')\n",
    "bad_dataset=[]\n",
    "for i, image_name in enumerate(bad_images):\n",
    "    if (image_name.split('.')[-1] == 'jpg'):\n",
    "        image = cv2.imread(image_directory + 'bad/' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        bad_dataset.append(np.array(image))\n",
    "bad_dataset = np.array(bad_dataset)\n",
    "bad_dataset = bad_dataset.astype('float32') / 255.\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 05:25:47.887957: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-05 05:25:47.888403: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-05-05 05:25:47.953463: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 536870912 exceeds 10% of free system memory.\n",
      "2023-05-05 05:25:48.055856: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 536870912 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 64)        3136      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 128)       131200    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 512)       1049088   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 131072)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              134218752 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 135,402,176\n",
      "Trainable params: 135,402,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 05:25:48.139967: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 536870912 exceeds 10% of free system memory.\n",
      "2023-05-05 05:25:48.248854: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 536870912 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "input shape:  (None, 128, 128, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 05:25:48.366845: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 536870912 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#Define the encoder - decoder network for input to the OutlierVAE detector class. \n",
    "#Can be any encoder and decoder. \n",
    "tf.keras.backend.clear_session()\n",
    "encoding_dim = 1024  #Dimension of the bottleneck encoder vector. \n",
    "# dense_dim = [8, 8, 512] #Dimension of the last conv. output. This is used to work our way back in the decoder. \n",
    "dense_dim = [16, 16, 512]\n",
    "#Define encoder\n",
    "encoder_net = tf.keras.Sequential(\n",
    "  [\n",
    "      InputLayer(input_shape=train[0].shape),                           # 128,128,3(input)\n",
    "      # filters, kernel_size, strides=(1, 1), padding=    \n",
    "      Conv2D(64, 4, strides=2, padding='same', activation=tf.nn.relu),  #>> 64*64*64\n",
    "      Conv2D(128, 4, strides=2, padding='same', activation=tf.nn.relu), #>> 32*32*128\n",
    "      Conv2D(512, 4, strides=2, padding='same', activation=tf.nn.relu), #>> 16*16*512\n",
    "      Flatten(),                                                          #>> 16x16x512=131072\n",
    "      Dense(encoding_dim,)                                                  #\n",
    "  ])\n",
    "\n",
    "print(encoder_net.summary())\n",
    "print(\"input shape: \",encoder_net.input_shape)\n",
    "\n",
    "#Define the decoder. \n",
    "#Start with the bottleneck dimension (encoder vector) and connect to dense layer \n",
    "#with dim = total nodes in the last conv. in the encoder. \n",
    "decoder_net = tf.keras.Sequential(\n",
    "  [\n",
    "      InputLayer(input_shape=(encoding_dim,)),\n",
    "      Dense(np.prod(dense_dim)),\n",
    "      Reshape(target_shape=dense_dim),\n",
    "      Conv2DTranspose(256, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "      Conv2DTranspose(64, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "      Conv2DTranspose(3, 4, strides=2, padding='same', activation='sigmoid')\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 131072)            134348800 \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 16, 16, 512)       0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 32, 32, 256)      2097408   \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 64, 64, 64)       262208    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 128, 128, 3)      3075      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 136,711,491\n",
      "Trainable params: 136,711,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "output shape (None, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_net.summary())\n",
    "print(\"output shape\",decoder_net.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current threshold value is:  0.015\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "#Define and train the outlier detector. \n",
    "\n",
    "latent_dim = 1024  #(Same as encoding dim. )\n",
    "\n",
    "# initialize outlier detector\n",
    "od = OutlierVAE(threshold=.015,  # threshold for outlier score above which the element is flagged as an outlier.\n",
    "                score_type='mse',  # use MSE of reconstruction error for outlier detection\n",
    "                encoder_net=encoder_net,  # can also pass VAE model instead\n",
    "                decoder_net=decoder_net,  # of separate encoder and decoder\n",
    "                latent_dim=latent_dim,\n",
    "                samples=4)\n",
    "\n",
    "print(\"Current threshold value is: \", od.threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 05:25:48.807617: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [10,128,128,3]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-05 05:25:48.807893: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [10,128,128,3]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [=] - 7s 7s/step - loss_ma: 213752.2812\n",
      "1/1 [=] - 3s 3s/step - loss_ma: 212159.7656\n",
      "1/1 [=] - 3s 3s/step - loss_ma: 210207.7812\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# train\n",
    "#from alibi_detect.models.tensorflow.losses import elbo #evidence lower bound loss\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "history = od.fit(train,\n",
    "                 optimizer = adam,\n",
    "                 epochs=10,\n",
    "                 batch_size=32,\n",
    "                 verbose=True)\n",
    "\n",
    "#Check the threshold value. Should be the same as defined before. \n",
    "print(\"Current threshold value is: \", od.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#infer_threshold Updates threshold by a value inferred from the percentage of \n",
    "#instances considered to be outliers in a sample of the dataset.\n",
    "#percentage of X considered to be normal based on the outlier score.\n",
    "#Here, we set it to 99%\n",
    "od.infer_threshold(test, outlier_type='instance', threshold_perc=99.0)\n",
    "print(\"Current threshold value is: \", od.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained outlier detector\n",
    "#As mentioned in their documentation, save and load is having issues in python3.6 but works fine in 3.7\n",
    "# from alibi_detect.utils import save_detector, load_detector  #If this does not work, try the next line\n",
    "# # from alibi_detect.utils.saving import save_detector, load_detector #Use this if the above line does not work. \n",
    "# save_detector(od, \"./saved_outlier_models/ocean-surface_od_20epochs.h5\")\n",
    "# od = load_detector(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test our model on a bad image\n",
    "img_num = 55\n",
    "test_bad_image = bad_dataset[img_num].reshape(1, SIZE, SIZE, 3)\n",
    "plt.imshow(test_bad_image[0])\n",
    "\n",
    "test_bad_image_recon = od.vae(test_bad_image)\n",
    "test_bad_image_recon = test_bad_image_recon.numpy()\n",
    "plt.imshow(test_bad_image_recon[0])\n",
    "\n",
    "test_bad_image_predict = od.predict(test_bad_image) #Returns a dictionary of data and metadata\n",
    "\n",
    "#Data dictionary contains the instance_score, feature_score, and whether it is an outlier or not. \n",
    "#Let u look at the values under the 'data' key in our output dictionary\n",
    "bad_image_instance_score = test_bad_image_predict['data']['instance_score'][0]\n",
    "print(\"The instance score is:\", bad_image_instance_score)\n",
    "\n",
    "bad_image_feature_score = test_bad_image_predict['data']['feature_score'][0]\n",
    "plt.imshow(bad_image_feature_score[:,:,0])\n",
    "print(\"Is this image an outlier (0 for NO and 1 for YES)?\", test_bad_image_predict['data']['is_outlier'][0])\n",
    "\n",
    "#You can also manually define the threshold based on your specific use case. \n",
    "# od.threshold = 0.002\n",
    "print(\"Current threshld value is: \", od.threshold)\n",
    "\n",
    "#Let us check it for multiple images\n",
    "X = bad_dataset[:]\n",
    "\n",
    "od_preds = od.predict(X,\n",
    "                      outlier_type='instance',    # use 'feature' or 'instance' level\n",
    "                      return_feature_score=True,  # scores used to determine outliers\n",
    "                      return_instance_score=True)\n",
    "\n",
    "print(list(od_preds['data'].keys()))\n",
    "\n",
    "#Scatter plot of instance scores. using the built-in function for the scatterplot. \n",
    "target = np.ones(X.shape[0],).astype(int)  # Ground truth (all ones for bad images)\n",
    "labels = ['normal', 'outlier']\n",
    "plot_instance_score(od_preds, target, labels, od.threshold) #pred, target, labels, threshold\n",
    "\n",
    "#Plot features for select images, using the built in function (plot_feature_outlier_image)\n",
    "X_recon = od.vae(X).numpy()\n",
    "plot_feature_outlier_image(od_preds,\n",
    "                           X,\n",
    "                           X_recon=X_recon,\n",
    "                           instance_ids=[21, 4, 9, 15, 66],  # pass a list with indices of instances to display\n",
    "                           max_instances=5,  # max nb of instances to display\n",
    "                           outliers_only=False)  # only show outlier predictions\n",
    "\n",
    "#######################################\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
