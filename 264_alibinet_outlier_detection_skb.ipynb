{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://youtu.be/Pql6ShORpNU\n",
    "\"\"\"\n",
    "Outlier detection using alibi-detect\n",
    "\n",
    "Alibi Detect is an open source Python library focused on outlier, adversarial and drift detection. \n",
    "The package aims to cover both online and offline detectors for tabular data, text, \n",
    "images and time series. The outlier detection methods should allow the user to \n",
    "identify global, contextual and collective outliers.\n",
    "\n",
    "pip install alibi-detect\n",
    "\n",
    "https://github.com/SeldonIO/alibi-detect\n",
    "Documentation: https://docs.seldon.io/_/downloads/alibi-detect/en/v0.5.1/pdf/\n",
    "\n",
    "We will be using VAE based outlier detection. Based on this paper:\n",
    "    https://arxiv.org/pdf/1312.6114.pdf\n",
    "    \n",
    "The Variational Auto-Encoder (VAE) outlier detector is first trained on a batch \n",
    "of unlabeled, but normal (inlier) data. Unsupervised training is desireable since \n",
    "labeled data is often scarce. The VAE detector tries to reconstruct the input it \n",
    "receives. If the input data cannot be reconstructed well, the reconstruction error \n",
    "is high and the data can be flagged as an outlier. The reconstruction error is either \n",
    "measured as the mean squared error (MSE) between the input and the reconstructed instance \n",
    "or as the probability that both the input and the reconstructed instance are \n",
    "generated by the same process.\n",
    "\n",
    "Data set info: https://openaccess.thecvf.com/content_CVPR_2019/papers/Bergmann_MVTec_AD_--_A_Comprehensive_Real-World_Dataset_for_Unsupervised_Anomaly_CVPR_2019_paper.pdf\n",
    "Data set link: https://www.mvtec.com/company/research/datasets/mvtec-ad\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Reshape, InputLayer, Flatten\n",
    "\n",
    "from alibi_detect.od import OutlierAE, OutlierVAE\n",
    "from alibi_detect.utils.visualize import plot_instance_score, plot_feature_outlier_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug note\n",
    "https://github.com/huggingface/transformers/issues/18549\n",
    "pip uninstall tokenizers\n",
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19321/301295471.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_directory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'bad/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "#Load data. We only need good data and anything NOT good is an outlier. \n",
    "\n",
    "image_directory = './datasets/'\n",
    "SIZE = 128\n",
    "dataset = []  #Many ways to handle data, you can use pandas. Here, we are using a list format.  \n",
    "\n",
    "good_images = os.listdir(image_directory + 'good/')\n",
    "for i, image_name in enumerate(good_images[:900]):\n",
    "    # print (image_name)\n",
    "    if (image_name.split('.')[-1] == 'jpg'):\n",
    "        image = cv2.imread(image_directory + 'good/' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "    \n",
    "\n",
    "dataset = np.array(dataset)\n",
    "\n",
    "train = dataset[0:700]\n",
    "test = dataset[700:]\n",
    "\n",
    "train = train.astype('float32') / 255.\n",
    "test = test.astype('float32') / 255.\n",
    "\n",
    "#Let us also load bad images to verify our trained model. \n",
    "bad_images = os.listdir(image_directory + 'bad')\n",
    "bad_dataset=[]\n",
    "for i, image_name in enumerate(bad_images):\n",
    "    if (image_name.split('.')[-1] == 'jpg'):\n",
    "        image = cv2.imread(image_directory + 'bad/' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        bad_dataset.append(np.array(image))\n",
    "bad_dataset = np.array(bad_dataset)\n",
    "bad_dataset = bad_dataset.astype('float32') / 255.\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "#Define the encoder - decoder network for input to the OutlierVAE detector class. \n",
    "#Can be any encoder and decoder. \n",
    "\n",
    "encoding_dim = 1024  #Dimension of the bottleneck encoder vector. \n",
    "# dense_dim = [8, 8, 512] #Dimension of the last conv. output. This is used to work our way back in the decoder. \n",
    "dense_dim = [16, 16, 512]\n",
    "#Define encoder\n",
    "encoder_net = tf.keras.Sequential(\n",
    "  [\n",
    "      InputLayer(input_shape=train[0].shape),                           # 128,128,3(input)\n",
    "      # filters, kernel_size, strides=(1, 1), padding=    \n",
    "      Conv2D(64, 4, strides=2, padding='same', activation=tf.nn.relu),  #>> 64*64*64\n",
    "      Conv2D(128, 4, strides=2, padding='same', activation=tf.nn.relu), #>> 32*32*128\n",
    "      Conv2D(512, 4, strides=2, padding='same', activation=tf.nn.relu), #>> 16*16*512\n",
    "      Flatten(),                                                          #>> 16x16x512=131072\n",
    "      Dense(encoding_dim,)                                                  #\n",
    "  ])\n",
    "\n",
    "print(encoder_net.summary())\n",
    "print(\"input shape: \",encoder_net.input_shape)\n",
    "\n",
    "#Define the decoder. \n",
    "#Start with the bottleneck dimension (encoder vector) and connect to dense layer \n",
    "#with dim = total nodes in the last conv. in the encoder. \n",
    "decoder_net = tf.keras.Sequential(\n",
    "  [\n",
    "      InputLayer(input_shape=(encoding_dim,)),\n",
    "      Dense(np.prod(dense_dim)),\n",
    "      Reshape(target_shape=dense_dim),\n",
    "      Conv2DTranspose(256, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "      Conv2DTranspose(64, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "      Conv2DTranspose(3, 4, strides=2, padding='same', activation='sigmoid')\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder_net.summary())\n",
    "print(\"output shape\",decoder_net.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#Define and train the outlier detector. \n",
    "\n",
    "latent_dim = 1024  #(Same as encoding dim. )\n",
    "\n",
    "# initialize outlier detector\n",
    "od = OutlierVAE(threshold=.015,  # threshold for outlier score above which the element is flagged as an outlier.\n",
    "                score_type='mse',  # use MSE of reconstruction error for outlier detection\n",
    "                encoder_net=encoder_net,  # can also pass VAE model instead\n",
    "                decoder_net=decoder_net,  # of separate encoder and decoder\n",
    "                latent_dim=latent_dim,\n",
    "                samples=4)\n",
    "\n",
    "print(\"Current threshold value is: \", od.threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "#from alibi_detect.models.tensorflow.losses import elbo #evidence lower bound loss\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "\n",
    "od.fit(train,\n",
    "       optimizer = adam,\n",
    "       epochs=20,\n",
    "       batch_size=50,\n",
    "       verbose=True)\n",
    "\n",
    "#Check the threshold value. Should be the same as defined before. \n",
    "print(\"Current threshold value is: \", od.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#infer_threshold Updates threshold by a value inferred from the percentage of \n",
    "#instances considered to be outliers in a sample of the dataset.\n",
    "#percentage of X considered to be normal based on the outlier score.\n",
    "#Here, we set it to 99%\n",
    "od.infer_threshold(test, outlier_type='instance', threshold_perc=99.0)\n",
    "print(\"Current threshold value is: \", od.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained outlier detector\n",
    "#As mentioned in their documentation, save and load is having issues in python3.6 but works fine in 3.7\n",
    "# from alibi_detect.utils import save_detector, load_detector  #If this does not work, try the next line\n",
    "# # from alibi_detect.utils.saving import save_detector, load_detector #Use this if the above line does not work. \n",
    "# save_detector(od, \"./saved_outlier_models/ocean-surface_od_20epochs.h5\")\n",
    "# od = load_detector(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test our model on a bad image\n",
    "img_num = 90\n",
    "test_bad_image = bad_dataset[img_num].reshape(1, 64, 64, 3)\n",
    "plt.imshow(test_bad_image[0])\n",
    "\n",
    "test_bad_image_recon = od.vae(test_bad_image)\n",
    "test_bad_image_recon = test_bad_image_recon.numpy()\n",
    "plt.imshow(test_bad_image_recon[0])\n",
    "\n",
    "test_bad_image_predict = od.predict(test_bad_image) #Returns a dictionary of data and metadata\n",
    "\n",
    "#Data dictionary contains the instance_score, feature_score, and whether it is an outlier or not. \n",
    "#Let u look at the values under the 'data' key in our output dictionary\n",
    "bad_image_instance_score = test_bad_image_predict['data']['instance_score'][0]\n",
    "print(\"The instance score is:\", bad_image_instance_score)\n",
    "\n",
    "bad_image_feature_score = test_bad_image_predict['data']['feature_score'][0]\n",
    "plt.imshow(bad_image_feature_score[:,:,0])\n",
    "print(\"Is this image an outlier (0 for NO and 1 for YES)?\", test_bad_image_predict['data']['is_outlier'][0])\n",
    "\n",
    "#You can also manually define the threshold based on your specific use case. \n",
    "od.threshold = 0.002\n",
    "print(\"Current threshld value is: \", od.threshold)\n",
    "\n",
    "#Let us check it for multiple images\n",
    "X = bad_dataset[:]\n",
    "\n",
    "od_preds = od.predict(X,\n",
    "                      outlier_type='instance',    # use 'feature' or 'instance' level\n",
    "                      return_feature_score=True,  # scores used to determine outliers\n",
    "                      return_instance_score=True)\n",
    "\n",
    "print(list(od_preds['data'].keys()))\n",
    "\n",
    "#Scatter plot of instance scores. using the built-in function for the scatterplot. \n",
    "target = np.ones(X.shape[0],).astype(int)  # Ground truth (all ones for bad images)\n",
    "labels = ['normal', 'outlier']\n",
    "plot_instance_score(od_preds, target, labels, od.threshold) #pred, target, labels, threshold\n",
    "\n",
    "#Plot features for select images, using the built in function (plot_feature_outlier_image)\n",
    "X_recon = od.vae(X).numpy()\n",
    "plot_feature_outlier_image(od_preds,\n",
    "                           X,\n",
    "                           X_recon=X_recon,\n",
    "                           instance_ids=[21, 4, 9, 15, 55],  # pass a list with indices of instances to display\n",
    "                           max_instances=5,  # max nb of instances to display\n",
    "                           outliers_only=False)  # only show outlier predictions\n",
    "\n",
    "#######################################\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
