{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://youtu.be/Pql6ShORpNU\n",
    "\"\"\"\n",
    "Outlier detection using alibi-detect\n",
    "\n",
    "Alibi Detect is an open source Python library focused on outlier, adversarial and drift detection. \n",
    "The package aims to cover both online and offline detectors for tabular data, text, \n",
    "images and time series. The outlier detection methods should allow the user to \n",
    "identify global, contextual and collective outliers.\n",
    "\n",
    "pip install alibi-detect\n",
    "\n",
    "https://github.com/SeldonIO/alibi-detect\n",
    "Documentation: https://docs.seldon.io/_/downloads/alibi-detect/en/v0.5.1/pdf/\n",
    "\n",
    "We will be using VAE based outlier detection. Based on this paper:\n",
    "    https://arxiv.org/pdf/1312.6114.pdf\n",
    "    \n",
    "The Variational Auto-Encoder (VAE) outlier detector is first trained on a batch \n",
    "of unlabeled, but normal (inlier) data. Unsupervised training is desireable since \n",
    "labeled data is often scarce. The VAE detector tries to reconstruct the input it \n",
    "receives. If the input data cannot be reconstructed well, the reconstruction error \n",
    "is high and the data can be flagged as an outlier. The reconstruction error is either \n",
    "measured as the mean squared error (MSE) between the input and the reconstructed instance \n",
    "or as the probability that both the input and the reconstructed instance are \n",
    "generated by the same process.\n",
    "\n",
    "Data set info: https://openaccess.thecvf.com/content_CVPR_2019/papers/Bergmann_MVTec_AD_--_A_Comprehensive_Real-World_Dataset_for_Unsupervised_Anomaly_CVPR_2019_paper.pdf\n",
    "Data set link: https://www.mvtec.com/company/research/datasets/mvtec-ad\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 04:29:24.331818: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-04 04:29:25.442254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n",
      "\n",
      "[KeOps] Warning : There were warnings or errors compiling formula :\n",
      "<stdin>:1:10: fatal error: cuda.h: No such file or directory\n",
      "compilation terminated.\n",
      "\n",
      "[KeOps] Warning : \n",
      "    The location of Cuda header files cuda.h and nvrtc.h could not be detected on your system.\n",
      "    You must determine their location and then define the environment variable CUDA_PATH,\n",
      "    either before launching Python or using os.environ before importing keops. For example\n",
      "    if these files are in /vol/cuda/10.2.89-cudnn7.6.4.38/include you can do :\n",
      "      import os\n",
      "      os.environ['CUDA_PATH'] = '/vol/cuda/10.2.89-cudnn7.6.4.38'\n",
      "    \n",
      "[KeOps] Compiling cuda jit compiler engine ... \n",
      "[KeOps] Warning : There were warnings or errors compiling formula :\n",
      "/home/skb/miniconda3/envs/jupyolo/lib/python3.10/site-packages/keopscore/binders/nvrtc/nvrtc_jit.cpp:5:10: fatal error: nvrtc.h: No such file or directory\n",
      "    5 | #include <nvrtc.h>\n",
      "      |          ^~~~~~~~~\n",
      "compilation terminated.\n",
      "\n",
      "OK\n",
      "[pyKeOps] Compiling nvrtc binder for python ... \n",
      "[KeOps] Warning : There were warnings or errors compiling formula :\n",
      "In file included from /home/skb/miniconda3/envs/jupyolo/lib/python3.10/site-packages/pykeops/common/keops_io/pykeops_nvrtc.cpp:4:\n",
      "/home/skb/miniconda3/envs/jupyolo/lib/python3.10/site-packages/keopscore/binders/nvrtc/keops_nvrtc.cpp:6:10: fatal error: nvrtc.h: No such file or directory\n",
      "    6 | #include <nvrtc.h>\n",
      "      |          ^~~~~~~~~\n",
      "compilation terminated.\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Reshape, InputLayer, Flatten\n",
    "\n",
    "from alibi_detect.od import OutlierAE, OutlierVAE\n",
    "from alibi_detect.utils.visualize import plot_instance_score, plot_feature_outlier_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug note\n",
    "https://github.com/huggingface/transformers/issues/18549\n",
    "pip uninstall tokenizers\n",
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 128, 128, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################################################\n",
    "#Load data. We only need good data and anything NOT good is an outlier. \n",
    "\n",
    "image_directory = './datasets/'\n",
    "SIZE = 128\n",
    "dataset = []  #Many ways to handle data, you can use pandas. Here, we are using a list format.  \n",
    "\n",
    "good_images = os.listdir(image_directory + 'good/')\n",
    "for i, image_name in enumerate(good_images[:900]):\n",
    "    # print (image_name)\n",
    "    if (image_name.split('.')[-1] == 'jpg'):\n",
    "        image = cv2.imread(image_directory + 'good/' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "    \n",
    "\n",
    "dataset = np.array(dataset)\n",
    "\n",
    "train = dataset[0:700]\n",
    "test = dataset[700:]\n",
    "\n",
    "train = train.astype('float32') / 255.\n",
    "test = test.astype('float32') / 255.\n",
    "\n",
    "#Let us also load bad images to verify our trained model. \n",
    "bad_images = os.listdir(image_directory + 'bad')\n",
    "bad_dataset=[]\n",
    "for i, image_name in enumerate(bad_images):\n",
    "    if (image_name.split('.')[-1] == 'jpg'):\n",
    "        image = cv2.imread(image_directory + 'bad/' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        bad_dataset.append(np.array(image))\n",
    "bad_dataset = np.array(bad_dataset)\n",
    "bad_dataset = bad_dataset.astype('float32') / 255.\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 04:30:43.400153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-04 04:30:43.430460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-04 04:30:43.430964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-04 04:30:43.433575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-04 04:30:43.434004: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-04 04:30:43.434334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-04 04:30:44.541079: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-04 04:30:44.541289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-04 04:30:44.541302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-05-04 04:30:44.541479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-04 04:30:44.541514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5903 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:06:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 64)        3136      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 128)       131200    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 512)       1049088   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 131072)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              134218752 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 135,402,176\n",
      "Trainable params: 135,402,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "input shape:  (None, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#Define the encoder - decoder network for input to the OutlierVAE detector class. \n",
    "#Can be any encoder and decoder. \n",
    "\n",
    "encoding_dim = 1024  #Dimension of the bottleneck encoder vector. \n",
    "# dense_dim = [8, 8, 512] #Dimension of the last conv. output. This is used to work our way back in the decoder. \n",
    "dense_dim = [16, 16, 512]\n",
    "#Define encoder\n",
    "encoder_net = tf.keras.Sequential(\n",
    "  [\n",
    "      InputLayer(input_shape=train[0].shape),                           # 128,128,3(input)\n",
    "      # filters, kernel_size, strides=(1, 1), padding=    \n",
    "      Conv2D(64, 4, strides=2, padding='same', activation=tf.nn.relu),  #>> 64*64*64\n",
    "      Conv2D(128, 4, strides=2, padding='same', activation=tf.nn.relu), #>> 32*32*128\n",
    "      Conv2D(512, 4, strides=2, padding='same', activation=tf.nn.relu), #>> 16*16*512\n",
    "      Flatten(),                                                          #>> 16x16x512=131072\n",
    "      Dense(encoding_dim,)                                                  #\n",
    "  ])\n",
    "\n",
    "print(encoder_net.summary())\n",
    "print(\"input shape: \",encoder_net.input_shape)\n",
    "\n",
    "#Define the decoder. \n",
    "#Start with the bottleneck dimension (encoder vector) and connect to dense layer \n",
    "#with dim = total nodes in the last conv. in the encoder. \n",
    "decoder_net = tf.keras.Sequential(\n",
    "  [\n",
    "      InputLayer(input_shape=(encoding_dim,)),\n",
    "      Dense(np.prod(dense_dim)),\n",
    "      Reshape(target_shape=dense_dim),\n",
    "      Conv2DTranspose(256, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "      Conv2DTranspose(64, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "      Conv2DTranspose(3, 4, strides=2, padding='same', activation='sigmoid')\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 131072)            134348800 \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 16, 16, 512)       0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 32, 32, 256)      2097408   \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 64, 64, 64)       262208    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 128, 128, 3)      3075      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 136,711,491\n",
      "Trainable params: 136,711,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "output shape (None, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_net.summary())\n",
    "print(\"output shape\",decoder_net.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current threshold value is:  0.015\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "#Define and train the outlier detector. \n",
    "\n",
    "latent_dim = 1024  #(Same as encoding dim. )\n",
    "\n",
    "# initialize outlier detector\n",
    "od = OutlierVAE(threshold=.015,  # threshold for outlier score above which the element is flagged as an outlier.\n",
    "                score_type='mse',  # use MSE of reconstruction error for outlier detection\n",
    "                encoder_net=encoder_net,  # can also pass VAE model instead\n",
    "                decoder_net=decoder_net,  # of separate encoder and decoder\n",
    "                latent_dim=latent_dim,\n",
    "                samples=4)\n",
    "\n",
    "print(\"Current threshold value is: \", od.threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "2023-05-04 04:30:45.491618: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 137625600 exceeds 10% of free system memory.\n",
      "2023-05-04 04:30:45.616219: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 137625600 exceeds 10% of free system memory.\n",
      "2023-05-04 04:30:45.690312: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 137625600 exceeds 10% of free system memory.\n",
      "2023-05-04 04:30:45.750055: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [700,128,128,3]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-04 04:30:45.750240: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [700,128,128,3]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-04 04:30:46.836510: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-05-04 04:30:46.837899: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at conv_ops.cc:1068 : UNIMPLEMENTED: DNN library is not found.\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Exception encountered when calling layer 'conv2d' (type Conv2D).\n\n{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} DNN library is not found. [Op:Conv2D]\n\nCall arguments received by layer 'conv2d' (type Conv2D):\n  • inputs=tf.Tensor(shape=(50, 128, 128, 3), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#from alibi_detect.models.tensorflow.losses import elbo #evidence lower bound loss\u001b[39;00m\n\u001b[1;32m      4\u001b[0m adam \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m history \u001b[39m=\u001b[39m od\u001b[39m.\u001b[39;49mfit(train,\n\u001b[1;32m      7\u001b[0m                  optimizer \u001b[39m=\u001b[39;49m adam,\n\u001b[1;32m      8\u001b[0m                  epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m                  batch_size\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m                  verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     12\u001b[0m \u001b[39m#Check the threshold value. Should be the same as defined before. \u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCurrent threshold value is: \u001b[39m\u001b[39m\"\u001b[39m, od\u001b[39m.\u001b[39mthreshold)\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyolo/lib/python3.10/site-packages/alibi_detect/od/vae.py:136\u001b[0m, in \u001b[0;36mOutlierVAE.fit\u001b[0;34m(self, X, loss_fn, optimizer, cov_elbo, epochs, batch_size, verbose, log_metric, callbacks)\u001b[0m\n\u001b[1;32m    133\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mloss_fn_kwargs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m {cov_elbo_type: tf\u001b[39m.\u001b[39mdtypes\u001b[39m.\u001b[39mcast(cov, tf\u001b[39m.\u001b[39mfloat32)}\n\u001b[1;32m    135\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m trainer(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyolo/lib/python3.10/site-packages/alibi_detect/models/tensorflow/trainer.py:83\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(model, loss_fn, x_train, y_train, dataset, optimizer, loss_fn_kwargs, preprocess_fn, epochs, reg_loss_fn, batch_size, buffer_size, verbose, log_metric, callbacks)\u001b[0m\n\u001b[1;32m     81\u001b[0m     x \u001b[39m=\u001b[39m preprocess_fn(x)\n\u001b[1;32m     82\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m---> 83\u001b[0m     y_hat \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     84\u001b[0m     y \u001b[39m=\u001b[39m x \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m y\n\u001b[1;32m     85\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(loss_fn, Callable):  \u001b[39m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyolo/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyolo/lib/python3.10/site-packages/alibi_detect/models/tensorflow/autoencoder.py:118\u001b[0m, in \u001b[0;36mVAE.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x: tf\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m tf\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 118\u001b[0m     z_mean, z_log_var, z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m    119\u001b[0m     x_recon \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(z)\n\u001b[1;32m    120\u001b[0m     \u001b[39m# add KL divergence loss term\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyolo/lib/python3.10/site-packages/alibi_detect/models/tensorflow/autoencoder.py:56\u001b[0m, in \u001b[0;36mEncoderVAE.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x: tf\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[tf\u001b[39m.\u001b[39mTensor, tf\u001b[39m.\u001b[39mTensor, tf\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m---> 56\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_net(x)\n\u001b[1;32m     57\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39mshape) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m     58\u001b[0m         x \u001b[39m=\u001b[39m Flatten()(x)\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Exception encountered when calling layer 'conv2d' (type Conv2D).\n\n{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} DNN library is not found. [Op:Conv2D]\n\nCall arguments received by layer 'conv2d' (type Conv2D):\n  • inputs=tf.Tensor(shape=(50, 128, 128, 3), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# train\n",
    "#from alibi_detect.models.tensorflow.losses import elbo #evidence lower bound loss\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "\n",
    "history = od.fit(train,\n",
    "                 optimizer = adam,\n",
    "                 epochs=20,\n",
    "                 batch_size=50,\n",
    "                 verbose=True)\n",
    "\n",
    "#Check the threshold value. Should be the same as defined before. \n",
    "print(\"Current threshold value is: \", od.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#infer_threshold Updates threshold by a value inferred from the percentage of \n",
    "#instances considered to be outliers in a sample of the dataset.\n",
    "#percentage of X considered to be normal based on the outlier score.\n",
    "#Here, we set it to 99%\n",
    "od.infer_threshold(test, outlier_type='instance', threshold_perc=99.0)\n",
    "print(\"Current threshold value is: \", od.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained outlier detector\n",
    "#As mentioned in their documentation, save and load is having issues in python3.6 but works fine in 3.7\n",
    "# from alibi_detect.utils import save_detector, load_detector  #If this does not work, try the next line\n",
    "# # from alibi_detect.utils.saving import save_detector, load_detector #Use this if the above line does not work. \n",
    "from alibi_detect.saving import save_detector\n",
    "filepath = './my_detector/'\n",
    "save_detector(od, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_detect.saving import load_detector\n",
    "\n",
    "filepath = './my_detector/'\n",
    "od = load_detector(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test our model on a bad image\n",
    "img_num = 90\n",
    "test_bad_image = bad_dataset[img_num].reshape(1, SIZE, SIZE, 3)\n",
    "plt.imshow(test_bad_image[0])\n",
    "\n",
    "test_bad_image_recon = od.vae(test_bad_image)\n",
    "test_bad_image_recon = test_bad_image_recon.numpy()\n",
    "plt.imshow(test_bad_image_recon[0])\n",
    "\n",
    "test_bad_image_predict = od.predict(test_bad_image) #Returns a dictionary of data and metadata\n",
    "\n",
    "#Data dictionary contains the instance_score, feature_score, and whether it is an outlier or not. \n",
    "#Let u look at the values under the 'data' key in our output dictionary\n",
    "bad_image_instance_score = test_bad_image_predict['data']['instance_score'][0]\n",
    "print(\"The instance score is:\", bad_image_instance_score)\n",
    "\n",
    "bad_image_feature_score = test_bad_image_predict['data']['feature_score'][0]\n",
    "plt.imshow(bad_image_feature_score[:,:,0])\n",
    "print(\"Is this image an outlier (0 for NO and 1 for YES)?\", test_bad_image_predict['data']['is_outlier'][0])\n",
    "\n",
    "#You can also manually define the threshold based on your specific use case. \n",
    "od.threshold = 0.002\n",
    "print(\"Current threshld value is: \", od.threshold)\n",
    "\n",
    "#Let us check it for multiple images\n",
    "X = bad_dataset[:]\n",
    "\n",
    "od_preds = od.predict(X,\n",
    "                      outlier_type='instance',    # use 'feature' or 'instance' level\n",
    "                      return_feature_score=True,  # scores used to determine outliers\n",
    "                      return_instance_score=True)\n",
    "\n",
    "print(list(od_preds['data'].keys()))\n",
    "\n",
    "#Scatter plot of instance scores. using the built-in function for the scatterplot. \n",
    "target = np.ones(X.shape[0],).astype(int)  # Ground truth (all ones for bad images)\n",
    "labels = ['normal', 'outlier']\n",
    "plot_instance_score(od_preds, target, labels, od.threshold) #pred, target, labels, threshold\n",
    "\n",
    "#Plot features for select images, using the built in function (plot_feature_outlier_image)\n",
    "X_recon = od.vae(X).numpy()\n",
    "plot_feature_outlier_image(od_preds,\n",
    "                           X,\n",
    "                           X_recon=X_recon,\n",
    "                           instance_ids=[21, 4, 9, 15, 55],  # pass a list with indices of instances to display\n",
    "                           max_instances=5,  # max nb of instances to display\n",
    "                           outliers_only=False)  # only show outlier predictions\n",
    "\n",
    "#######################################\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
