{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://youtu.be/Pql6ShORpNU\n",
    "\"\"\"\n",
    "Outlier detection using alibi-detect\n",
    "\n",
    "Alibi Detect is an open source Python library focused on outlier, adversarial and drift detection. \n",
    "The package aims to cover both online and offline detectors for tabular data, text, \n",
    "images and time series. The outlier detection methods should allow the user to \n",
    "identify global, contextual and collective outliers.\n",
    "\n",
    "pip install alibi-detect\n",
    "\n",
    "https://github.com/SeldonIO/alibi-detect\n",
    "Documentation: https://docs.seldon.io/_/downloads/alibi-detect/en/v0.5.1/pdf/\n",
    "\n",
    "We will be using VAE based outlier detection. Based on this paper:\n",
    "    https://arxiv.org/pdf/1312.6114.pdf\n",
    "    \n",
    "The Variational Auto-Encoder (VAE) outlier detector is first trained on a batch \n",
    "of unlabeled, but normal (inlier) data. Unsupervised training is desireable since \n",
    "labeled data is often scarce. The VAE detector tries to reconstruct the input it \n",
    "receives. If the input data cannot be reconstructed well, the reconstruction error \n",
    "is high and the data can be flagged as an outlier. The reconstruction error is either \n",
    "measured as the mean squared error (MSE) between the input and the reconstructed instance \n",
    "or as the probability that both the input and the reconstructed instance are \n",
    "generated by the same process.\n",
    "\n",
    "Data set info: https://openaccess.thecvf.com/content_CVPR_2019/papers/Bergmann_MVTec_AD_--_A_Comprehensive_Real-World_Dataset_for_Unsupervised_Anomaly_CVPR_2019_paper.pdf\n",
    "Data set link: https://www.mvtec.com/company/research/datasets/mvtec-ad\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_CPP_MIN_LOG_LEVEL=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 04:32:09.560704: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "%env TF_CPP_MIN_LOG_LEVEL=2\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "# tf.keras.backend.set_floatx('float16')\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Reshape, InputLayer, Flatten\n",
    "\n",
    "from alibi_detect.od import OutlierAE, OutlierVAE\n",
    "from alibi_detect.utils.visualize import plot_instance_score, plot_feature_outlier_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug note\n",
    "https://github.com/huggingface/transformers/issues/18549\n",
    "pip uninstall tokenizers\n",
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 128, 128, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################################################\n",
    "#Load data. We only need good data and anything NOT good is an outlier. \n",
    "\n",
    "image_directory = './datasets/'\n",
    "SIZE = 128\n",
    "dataset = []  #Many ways to handle data, you can use pandas. Here, we are using a list format.  \n",
    "\n",
    "good_images = os.listdir(image_directory + 'good/')\n",
    "for i, image_name in enumerate(good_images[:900]):\n",
    "    # print (image_name)\n",
    "    if (image_name.split('.')[-1] == 'jpg'):\n",
    "        image = cv2.imread(image_directory + 'good/' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "    \n",
    "\n",
    "dataset = np.array(dataset)\n",
    "\n",
    "train = dataset[0:700]\n",
    "test = dataset[700:]\n",
    "\n",
    "train = train.astype('float32') / 255.\n",
    "test = test.astype('float32') / 255.\n",
    "\n",
    "#Let us also load bad images to verify our trained model. \n",
    "bad_images = os.listdir(image_directory + 'bad')\n",
    "bad_dataset=[]\n",
    "for i, image_name in enumerate(bad_images):\n",
    "    if (image_name.split('.')[-1] == 'jpg'):\n",
    "        image = cv2.imread(image_directory + 'bad/' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        bad_dataset.append(np.array(image))\n",
    "bad_dataset = np.array(bad_dataset)\n",
    "bad_dataset = bad_dataset.astype('float32') / 255.\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 64)        3136      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 128)       131200    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 512)       1049088   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 131072)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              134218752 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 135,402,176\n",
      "Trainable params: 135,402,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "input shape:  (None, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#Define the encoder - decoder network for input to the OutlierVAE detector class. \n",
    "#Can be any encoder and decoder. \n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "encoding_dim = 1024  #Dimension of the bottleneck encoder vector. \n",
    "# dense_dim = [8, 8, 512] #Dimension of the last conv. output. This is used to work our way back in the decoder. \n",
    "dense_dim = [16, 16, 512]\n",
    "#Define encoder\n",
    "encoder_net = tf.keras.Sequential(\n",
    "  [\n",
    "      InputLayer(input_shape=train[0].shape),                           # 128,128,3(input)\n",
    "      # filters, kernel_size, strides=(1, 1), padding=    \n",
    "      Conv2D(64, 4, strides=2, padding='same', activation=tf.nn.relu),  #>> 64*64*64\n",
    "      Conv2D(128, 4, strides=2, padding='same', activation=tf.nn.relu), #>> 32*32*128\n",
    "      Conv2D(512, 4, strides=2, padding='same', activation=tf.nn.relu), #>> 16*16*512\n",
    "      Flatten(),                                                          #>> 16x16x512=131072\n",
    "      Dense(encoding_dim,)                                                  #\n",
    "  ])\n",
    "\n",
    "print(encoder_net.summary())\n",
    "print(\"input shape: \",encoder_net.input_shape)\n",
    "\n",
    "#Define the decoder. \n",
    "#Start with the bottleneck dimension (encoder vector) and connect to dense layer \n",
    "#with dim = total nodes in the last conv. in the encoder. \n",
    "decoder_net = tf.keras.Sequential(\n",
    "  [\n",
    "      InputLayer(input_shape=(encoding_dim,)),\n",
    "      Dense(np.prod(dense_dim)),\n",
    "      Reshape(target_shape=dense_dim),\n",
    "      Conv2DTranspose(256, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "      Conv2DTranspose(64, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "      Conv2DTranspose(3, 4, strides=2, padding='same', activation='sigmoid')\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 131072)            134348800 \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 16, 16, 512)       0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 32, 32, 256)      2097408   \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 64, 64, 64)       262208    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 128, 128, 3)      3075      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 136,711,491\n",
      "Trainable params: 136,711,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "output shape (None, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_net.summary())\n",
    "print(\"output shape\",decoder_net.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current threshold value is:  0.015\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "#Define and train the outlier detector. \n",
    "\n",
    "latent_dim = 1024  #(Same as encoding dim. )\n",
    "\n",
    "# initialize outlier detector\n",
    "od = OutlierVAE(threshold=.015,  # threshold for outlier score above which the element is flagged as an outlier.\n",
    "                score_type='mse',  # use MSE of reconstruction error for outlier detection\n",
    "                encoder_net=encoder_net,  # can also pass VAE model instead\n",
    "                decoder_net=decoder_net,  # of separate encoder and decoder\n",
    "                latent_dim=latent_dim,\n",
    "                samples=4)\n",
    "\n",
    "print(\"Current threshold value is: \", od.threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skb/miniconda3/envs/tf/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [=] - 5s 141ms/step - loss_ma: 464616.1243\n",
      " 4/14 [.] - ETA: 1s - loss_ma: 245474.5713"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__Conv2DBackpropInput_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[50,64,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2DBackpropInput]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#from alibi_detect.models.tensorflow.losses import elbo #evidence lower bound loss\u001b[39;00m\n\u001b[1;32m      4\u001b[0m adam \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m history \u001b[39m=\u001b[39m od\u001b[39m.\u001b[39;49mfit(train,\n\u001b[1;32m      7\u001b[0m                  optimizer \u001b[39m=\u001b[39;49m adam,\n\u001b[1;32m      8\u001b[0m                  epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m                  batch_size\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m                  verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     12\u001b[0m \u001b[39m#Check the threshold value. Should be the same as defined before. \u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCurrent threshold value is: \u001b[39m\u001b[39m\"\u001b[39m, od\u001b[39m.\u001b[39mthreshold)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/alibi_detect/od/vae.py:136\u001b[0m, in \u001b[0;36mOutlierVAE.fit\u001b[0;34m(self, X, loss_fn, optimizer, cov_elbo, epochs, batch_size, verbose, log_metric, callbacks)\u001b[0m\n\u001b[1;32m    133\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mloss_fn_kwargs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m {cov_elbo_type: tf\u001b[39m.\u001b[39mdtypes\u001b[39m.\u001b[39mcast(cov, tf\u001b[39m.\u001b[39mfloat32)}\n\u001b[1;32m    135\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m trainer(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/alibi_detect/models/tensorflow/trainer.py:94\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(model, loss_fn, x_train, y_train, dataset, optimizer, loss_fn_kwargs, preprocess_fn, epochs, reg_loss_fn, batch_size, buffer_size, verbose, log_metric, callbacks)\u001b[0m\n\u001b[1;32m     91\u001b[0m         loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(model\u001b[39m.\u001b[39mlosses)\n\u001b[1;32m     92\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reg_loss_fn(model)  \u001b[39m# alternative way they might be specified\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(loss, model\u001b[39m.\u001b[39;49mtrainable_weights)\n\u001b[1;32m     95\u001b[0m optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(grads, model\u001b[39m.\u001b[39mtrainable_weights))\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:1113\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1107\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[1;32m   1108\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1109\u001b[0m           output_gradients))\n\u001b[1;32m   1110\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1111\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1113\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[1;32m   1114\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[1;32m   1115\u001b[0m     flat_targets,\n\u001b[1;32m   1116\u001b[0m     flat_sources,\n\u001b[1;32m   1117\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[1;32m   1118\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[1;32m   1119\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[1;32m   1121\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[1;32m   1122\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:160\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    158\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[1;32m    161\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/ops/nn_grad.py:581\u001b[0m, in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    572\u001b[0m shape_0, shape_1 \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39mshape_n([op\u001b[39m.\u001b[39minputs[\u001b[39m0\u001b[39m], op\u001b[39m.\u001b[39minputs[\u001b[39m1\u001b[39m]])\n\u001b[1;32m    574\u001b[0m \u001b[39m# We call the gen_nn_ops backprop functions instead of nn_ops backprop\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[39m# functions for performance reasons in Eager mode. gen_nn_ops functions take a\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[39m# `explicit_paddings` parameter, but nn_ops functions do not. So if we were\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[39m# to use the nn_ops functions, we would have to convert `padding` and\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \u001b[39m# `explicit_paddings` into a single `padding` parameter, increasing overhead\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39m# in Eager mode.\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m--> 581\u001b[0m     gen_nn_ops\u001b[39m.\u001b[39;49mconv2d_backprop_input(\n\u001b[1;32m    582\u001b[0m         shape_0,\n\u001b[1;32m    583\u001b[0m         op\u001b[39m.\u001b[39;49minputs[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m    584\u001b[0m         grad,\n\u001b[1;32m    585\u001b[0m         dilations\u001b[39m=\u001b[39;49mdilations,\n\u001b[1;32m    586\u001b[0m         strides\u001b[39m=\u001b[39;49mstrides,\n\u001b[1;32m    587\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m    588\u001b[0m         explicit_paddings\u001b[39m=\u001b[39;49mexplicit_paddings,\n\u001b[1;32m    589\u001b[0m         use_cudnn_on_gpu\u001b[39m=\u001b[39;49muse_cudnn_on_gpu,\n\u001b[1;32m    590\u001b[0m         data_format\u001b[39m=\u001b[39;49mdata_format),\n\u001b[1;32m    591\u001b[0m     gen_nn_ops\u001b[39m.\u001b[39mconv2d_backprop_filter(\n\u001b[1;32m    592\u001b[0m         op\u001b[39m.\u001b[39minputs[\u001b[39m0\u001b[39m],\n\u001b[1;32m    593\u001b[0m         shape_1,\n\u001b[1;32m    594\u001b[0m         grad,\n\u001b[1;32m    595\u001b[0m         dilations\u001b[39m=\u001b[39mdilations,\n\u001b[1;32m    596\u001b[0m         strides\u001b[39m=\u001b[39mstrides,\n\u001b[1;32m    597\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m    598\u001b[0m         explicit_paddings\u001b[39m=\u001b[39mexplicit_paddings,\n\u001b[1;32m    599\u001b[0m         use_cudnn_on_gpu\u001b[39m=\u001b[39muse_cudnn_on_gpu,\n\u001b[1;32m    600\u001b[0m         data_format\u001b[39m=\u001b[39mdata_format)\n\u001b[1;32m    601\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/ops/gen_nn_ops.py:1423\u001b[0m, in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1421\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   1422\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1423\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[1;32m   1424\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[1;32m   1425\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7208\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 7209\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__Conv2DBackpropInput_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[50,64,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2DBackpropInput]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# train\n",
    "#from alibi_detect.models.tensorflow.losses import elbo #evidence lower bound loss\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "\n",
    "history = od.fit(train,\n",
    "                 optimizer = adam,\n",
    "                 epochs=2,\n",
    "                 batch_size=50,\n",
    "                 verbose=True)\n",
    "\n",
    "#Check the threshold value. Should be the same as defined before. \n",
    "print(\"Current threshold value is: \", od.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#infer_threshold Updates threshold by a value inferred from the percentage of \n",
    "#instances considered to be outliers in a sample of the dataset.\n",
    "#percentage of X considered to be normal based on the outlier score.\n",
    "#Here, we set it to 99%\n",
    "od.infer_threshold(test, outlier_type='instance', threshold_perc=99.0)\n",
    "print(\"Current threshold value is: \", od.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained outlier detector\n",
    "#As mentioned in their documentation, save and load is having issues in python3.6 but works fine in 3.7\n",
    "# from alibi_detect.utils import save_detector, load_detector  #If this does not work, try the next line\n",
    "# # from alibi_detect.utils.saving import save_detector, load_detector #Use this if the above line does not work. \n",
    "from alibi_detect.saving import save_detector\n",
    "# filepath = './my_detector/'\n",
    "# save_detector(od, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_detect.saving import load_detector\n",
    "\n",
    "# filepath = './my_detector/'\n",
    "# od = load_detector(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test our model on a bad image\n",
    "img_num = 90\n",
    "test_bad_image = bad_dataset[img_num].reshape(1, SIZE, SIZE, 3)\n",
    "plt.imshow(test_bad_image[0])\n",
    "\n",
    "test_bad_image_recon = od.vae(test_bad_image)\n",
    "test_bad_image_recon = test_bad_image_recon.numpy()\n",
    "plt.imshow(test_bad_image_recon[0])\n",
    "\n",
    "test_bad_image_predict = od.predict(test_bad_image) #Returns a dictionary of data and metadata\n",
    "\n",
    "#Data dictionary contains the instance_score, feature_score, and whether it is an outlier or not. \n",
    "#Let u look at the values under the 'data' key in our output dictionary\n",
    "bad_image_instance_score = test_bad_image_predict['data']['instance_score'][0]\n",
    "print(\"The instance score is:\", bad_image_instance_score)\n",
    "\n",
    "bad_image_feature_score = test_bad_image_predict['data']['feature_score'][0]\n",
    "plt.imshow(bad_image_feature_score[:,:,0])\n",
    "print(\"Is this image an outlier (0 for NO and 1 for YES)?\", test_bad_image_predict['data']['is_outlier'][0])\n",
    "\n",
    "#You can also manually define the threshold based on your specific use case. \n",
    "od.threshold = 0.002\n",
    "print(\"Current threshld value is: \", od.threshold)\n",
    "\n",
    "#Let us check it for multiple images\n",
    "X = bad_dataset[:]\n",
    "\n",
    "od_preds = od.predict(X,\n",
    "                      outlier_type='instance',    # use 'feature' or 'instance' level\n",
    "                      return_feature_score=True,  # scores used to determine outliers\n",
    "                      return_instance_score=True)\n",
    "\n",
    "print(list(od_preds['data'].keys()))\n",
    "\n",
    "#Scatter plot of instance scores. using the built-in function for the scatterplot. \n",
    "target = np.ones(X.shape[0],).astype(int)  # Ground truth (all ones for bad images)\n",
    "labels = ['normal', 'outlier']\n",
    "plot_instance_score(od_preds, target, labels, od.threshold) #pred, target, labels, threshold\n",
    "\n",
    "#Plot features for select images, using the built in function (plot_feature_outlier_image)\n",
    "X_recon = od.vae(X).numpy()\n",
    "plot_feature_outlier_image(od_preds,\n",
    "                           X,\n",
    "                           X_recon=X_recon,\n",
    "                           instance_ids=[21, 4, 9, 15, 55],  # pass a list with indices of instances to display\n",
    "                           max_instances=5,  # max nb of instances to display\n",
    "                           outliers_only=False)  # only show outlier predictions\n",
    "\n",
    "#######################################\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
